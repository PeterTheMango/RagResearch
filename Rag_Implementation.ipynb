{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V5E1",
   "authorship_tag": "ABX9TyNHdVTJRVJPL9F3Gx9lecx0",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PeterTheMango/RagResearch/blob/main/Rag_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building a RAG System\n",
    "### Done by: Peter Sotomango [60301211]\n",
    "\n",
    "In this notebook I explored how to design a RAG based Q/A system and used embedding models and large language models from Hugging Face.\n",
    "\n",
    "I focused on using lightweight models for now due to limited resource constraints.\n",
    "\n",
    "The dateset that was used was [G4KMU's T2 - RagBench](https://huggingface.co/datasets/G4KMU/t2-ragbench) for getting test documents to put in the database and test the LLMs."
   ],
   "metadata": {
    "id": "sSnMPs7v9_TY"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# CLAUDE PLANNING\n\n## Project Overview\nThis section contains the implementation plan for the RAG (Retrieval-Augmented Generation) Q&A system.\n\n## Current Status\n- âœ… Planning phase complete\n- Ready for implementation\n\n---\n\n## Architecture Decisions\n\n### 1. **Device Management**\n- Auto-detect GPU availability using `torch.cuda.is_available()`\n- Fallback to CPU if GPU unavailable\n- Move models to appropriate device automatically\n\n### 2. **Models**\n\n#### Embedding Model (Recommended: BAAI/bge-small-en-v1.5)\n**Primary Choice:**\n- **BAAI/bge-small-en-v1.5** \n  - Size: 33M parameters, 384 dimensions\n  - Excellent performance-to-size ratio\n  - Good for both CPU and GPU\n\n**Alternatives:**\n- **sentence-transformers/all-MiniLM-L6-v2** (faster, 22M params, 384 dims)\n- **BAAI/bge-base-en-v1.5** (better quality, 109M params, 768 dims - GPU preferred)\n\n#### LLM (Language Model)\n**Primary Choice:**\n- **mistralai/Mistral-7B-Instruct-v0.2** (GPU recommended)\n  - 7B parameters\n  - Strong instruction following\n  - Good balance of quality and speed\n\n**Alternatives:**\n- **google/flan-t5-large** (780M params - lighter)\n- **TinyLlama/TinyLlama-1.1B-Chat-v1.0** (1.1B params - CPU friendly)\n\n### 3. **Vector Database**\n- **ChromaDB** - Simple, lightweight, persistent storage\n- Local storage for embeddings\n- Supports similarity search with various distance metrics\n\n### 4. **Data Source**\n- PDFs from `data/` folder\n- Subset of G4KMU T2-RagBench dataset\n- Dynamic loading - add PDFs as needed\n\n### 5. **Chunking Strategy**\n- **Semantic Chunking** with sentence-level splitting\n- Approach: Use sentence boundaries as natural breakpoints\n- Recommended: RecursiveCharacterTextSplitter with sentence separators\n- Target chunk size: 512-1024 characters (adjustable based on model context)\n- Overlap: 50-100 characters to maintain context continuity\n\n**Alternative Approaches:**\n- Fixed-size chunking (simpler but less semantic)\n- Paragraph-based chunking (larger chunks)\n- Sliding window with larger overlap\n\n### 6. **Retrieval Strategy**\n- **Similarity Search** using cosine similarity\n- Top-k retrieval (k=3-5 most relevant chunks)\n- Return chunks with similarity scores\n\n**Future Enhancements:**\n- Re-ranking with cross-encoder\n- Hybrid search (keyword + semantic)\n- MMR (Maximal Marginal Relevance) for diversity\n\n### 7. **Evaluation Metrics**\n\n#### RAG-Specific Metrics:\n1. **Context Relevance** - How relevant are retrieved documents to the query?\n2. **Answer Relevance** - How relevant is the generated answer to the query?\n3. **Faithfulness/Groundedness** - Is the answer consistent with retrieved context?\n4. **Context Precision** - Precision of relevant chunks in top-k results\n5. **Context Recall** - Coverage of relevant information\n\n#### Retrieval Metrics:\n- **Hit Rate** - Percentage of queries with at least one relevant result\n- **MRR (Mean Reciprocal Rank)** - Average of reciprocal ranks of first relevant result\n- **Similarity Scores** - Average cosine similarity of retrieved chunks\n\n#### Answer Quality Metrics:\n- **Answer Similarity** - Semantic similarity to ground truth (if available)\n- **Response Time** - Latency for end-to-end query processing\n- **BLEU/ROUGE** (optional) - If reference answers available\n\n---\n\n## Implementation Plan\n\n### Phase 1: Environment Setup\n1. Install required packages:\n   - `transformers`, `sentence-transformers`, `torch`\n   - `chromadb`\n   - `PyPDF2` or `pypdf` for PDF processing\n   - `langchain` (optional, for text splitting utilities)\n   - `nltk` or `spacy` for sentence tokenization\n\n2. Set up device detection and configuration\n3. Create data/ folder structure\n\n### Phase 2: Data Ingestion & Processing\n1. **Load PDFs** from data/ folder\n   - Extract text from each PDF\n   - Maintain document metadata (filename, page numbers)\n\n2. **Chunk Documents**\n   - Implement semantic chunking with sentence boundaries\n   - Create chunk metadata (source document, chunk index, page number)\n   - Store original text alongside chunks\n\n3. **Generate Embeddings**\n   - Load embedding model (BAAI/bge-small-en-v1.5)\n   - Batch process chunks for efficiency\n   - Generate embeddings for all chunks\n\n4. **Store in ChromaDB**\n   - Initialize ChromaDB collection\n   - Store embeddings with metadata\n   - Create persistent storage\n\n### Phase 3: RAG Query Pipeline\n1. **Load Models**\n   - Load embedding model for query encoding\n   - Load LLM for answer generation\n   - Configure generation parameters\n\n2. **Query Processing**\n   - Accept user question\n   - Generate query embedding\n   - Retrieve top-k similar chunks from ChromaDB\n\n3. **Answer Generation**\n   - Construct prompt with retrieved context\n   - Format: \"Context: {chunks}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n   - Generate answer using LLM\n   - Return answer with sources and similarity scores\n\n### Phase 4: Evaluation & Metrics\n1. **Implement Metric Calculators**\n   - Context relevance scorer\n   - Answer relevance scorer\n   - Faithfulness checker\n   - Retrieval metrics (Hit Rate, MRR)\n\n2. **Logging & Output**\n   - Log queries, retrieved contexts, and answers\n   - Save evaluation metrics to file\n   - Create visualization of results (optional)\n\n3. **Test Cases**\n   - Create test questions for evaluation\n   - Compare results across different configurations\n\n---\n\n## Notes & Considerations\n\n### Performance Optimization:\n- Use batch processing for embeddings\n- Consider quantization (4-bit/8-bit) for LLM if memory constrained\n- Cache embeddings to avoid recomputation\n- Use GPU memory efficiently (offload when not in use)\n\n### Quality Improvements:\n- Experiment with different chunk sizes\n- Tune top-k retrieval parameter\n- Try different prompt templates\n- Consider re-ranking retrieved results\n\n### Future Enhancements:\n- Add query expansion/reformulation\n- Implement conversational memory for multi-turn QA\n- Add citation/source attribution in answers\n- Support multiple embedding models comparison\n- Web interface for easier interaction\n\n### Error Handling:\n- Handle missing PDFs gracefully\n- Validate embedding dimensions\n- Catch model loading errors\n- Log failures for debugging\n\n---\n\n## Dependencies\n```python\n# Core ML\ntorch\ntransformers\nsentence-transformers\n\n# Vector DB\nchromadb\n\n# Text Processing\npypdf or PyPDF2\nlangchain or langchain-text-splitters\nnltk\n\n# Evaluation\nscikit-learn (for metrics)\nnumpy\npandas\n\n# Optional\nragas (for advanced RAG metrics)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ENVIRONMENT CONFIGURATION"
   ],
   "metadata": {
    "id": "IUnk5MPZJImN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ingesting Data\n"
   ],
   "metadata": {
    "id": "8tMHfg4SJIwF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process Data"
   ],
   "metadata": {
    "id": "FVN7SosbJJEn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save to vector database"
   ],
   "metadata": {
    "id": "QCi3ny5uLnNP"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "M6nSuiYtL-VD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "tZHgW-JBL-XW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Models"
   ],
   "metadata": {
    "id": "C3avhryaL-l7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get User Question"
   ],
   "metadata": {
    "id": "-IcSSacRMGtJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Model"
   ],
   "metadata": {
    "id": "wVYR3nsQMHhO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Output"
   ],
   "metadata": {
    "id": "G5S08vRTMI3g"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Outputs"
   ],
   "metadata": {
    "id": "_sapiUdeMMm_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics"
   ],
   "metadata": {
    "id": "HcWpIrA9MM4c"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "P78NpzbYMFgh"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
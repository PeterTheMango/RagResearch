{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V5E1",
   "authorship_tag": "ABX9TyNHdVTJRVJPL9F3Gx9lecx0",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PeterTheMango/RagResearch/blob/main/Rag_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building a RAG System\n",
    "### Done by: Peter Sotomango [60301211]\n",
    "\n",
    "In this notebook I explored how to design a RAG based Q/A system and used embedding models and large language models from Hugging Face.\n",
    "\n",
    "I focused on using lightweight models for now due to limited resource constraints.\n",
    "\n",
    "The dateset that was used was [G4KMU's T2 - RagBench](https://huggingface.co/datasets/G4KMU/t2-ragbench) for getting test documents to put in the database and test the LLMs."
   ],
   "metadata": {
    "id": "sSnMPs7v9_TY"
   }
  },
  {
   "cell_type": "markdown",
   "source": "# CLAUDE PLANNING\n\n## Project Overview\nThis section contains the implementation plan for the RAG (Retrieval-Augmented Generation) Q&A system.\n\n## Current Status\n- âœ… Planning phase complete\n- Ready for implementation\n\n---\n\n## Architecture Decisions\n\n### 1. **Device Management**\n- Auto-detect GPU availability using `torch.cuda.is_available()`\n- Fallback to CPU if GPU unavailable\n- Move models to appropriate device automatically\n\n### 2. **Models**\n\n#### Embedding Model (Recommended: BAAI/bge-small-en-v1.5)\n**Primary Choice:**\n- **BAAI/bge-small-en-v1.5** \n  - Size: 33M parameters, 384 dimensions\n  - Excellent performance-to-size ratio\n  - Good for both CPU and GPU\n\n**Alternatives:**\n- **sentence-transformers/all-MiniLM-L6-v2** (faster, 22M params, 384 dims)\n- **BAAI/bge-base-en-v1.5** (better quality, 109M params, 768 dims - GPU preferred)\n\n#### LLM (Language Model)\n**Primary Choice:**\n- **mistralai/Mistral-7B-Instruct-v0.2** (GPU recommended)\n  - 7B parameters\n  - Strong instruction following\n  - Good balance of quality and speed\n\n**Alternatives:**\n- **google/flan-t5-large** (780M params - lighter)\n- **TinyLlama/TinyLlama-1.1B-Chat-v1.0** (1.1B params - CPU friendly)\n\n### 3. **Vector Database**\n- **ChromaDB** - Simple, lightweight, persistent storage\n- Local storage for embeddings\n- Supports similarity search with various distance metrics\n\n### 4. **Data Source**\n- PDFs from `data/` folder\n- Subset of G4KMU T2-RagBench dataset\n- Dynamic loading - add PDFs as needed\n\n### 5. **Chunking Strategy**\n- **Semantic Chunking** with sentence-level splitting\n- Approach: Use sentence boundaries as natural breakpoints\n- Recommended: RecursiveCharacterTextSplitter with sentence separators\n- Target chunk size: 512-1024 characters (adjustable based on model context)\n- Overlap: 50-100 characters to maintain context continuity\n\n**Alternative Approaches:**\n- Fixed-size chunking (simpler but less semantic)\n- Paragraph-based chunking (larger chunks)\n- Sliding window with larger overlap\n\n### 6. **Retrieval Strategy**\n- **Similarity Search** using cosine similarity\n- Top-k retrieval (k=3-5 most relevant chunks)\n- Return chunks with similarity scores\n\n**Future Enhancements:**\n- Re-ranking with cross-encoder\n- Hybrid search (keyword + semantic)\n- MMR (Maximal Marginal Relevance) for diversity\n\n### 7. **Evaluation Metrics**\n\n#### RAG-Specific Metrics:\n1. **Context Relevance** - How relevant are retrieved documents to the query?\n2. **Answer Relevance** - How relevant is the generated answer to the query?\n3. **Faithfulness/Groundedness** - Is the answer consistent with retrieved context?\n4. **Context Precision** - Precision of relevant chunks in top-k results\n5. **Context Recall** - Coverage of relevant information\n\n#### Retrieval Metrics:\n- **Hit Rate** - Percentage of queries with at least one relevant result\n- **MRR (Mean Reciprocal Rank)** - Average of reciprocal ranks of first relevant result\n- **Similarity Scores** - Average cosine similarity of retrieved chunks\n\n#### Answer Quality Metrics:\n- **Answer Similarity** - Semantic similarity to ground truth (if available)\n- **Response Time** - Latency for end-to-end query processing\n- **BLEU/ROUGE** (optional) - If reference answers available\n\n---\n\n## Implementation Plan\n\n### Phase 1: Environment Setup\n1. Install required packages:\n   - `transformers`, `sentence-transformers`, `torch`\n   - `chromadb`\n   - `PyPDF2` or `pypdf` for PDF processing\n   - `langchain` (optional, for text splitting utilities)\n   - `nltk` or `spacy` for sentence tokenization\n\n2. Set up device detection and configuration\n3. Create data/ folder structure\n\n### Phase 2: Data Ingestion & Processing\n1. **Load PDFs** from data/ folder\n   - Extract text from each PDF\n   - Maintain document metadata (filename, page numbers)\n\n2. **Chunk Documents**\n   - Implement semantic chunking with sentence boundaries\n   - Create chunk metadata (source document, chunk index, page number)\n   - Store original text alongside chunks\n\n3. **Generate Embeddings**\n   - Load embedding model (BAAI/bge-small-en-v1.5)\n   - Batch process chunks for efficiency\n   - Generate embeddings for all chunks\n\n4. **Store in ChromaDB**\n   - Initialize ChromaDB collection\n   - Store embeddings with metadata\n   - Create persistent storage\n\n### Phase 3: RAG Query Pipeline\n1. **Load Models**\n   - Load embedding model for query encoding\n   - Load LLM for answer generation\n   - Configure generation parameters\n\n2. **Query Processing**\n   - Accept user question\n   - Generate query embedding\n   - Retrieve top-k similar chunks from ChromaDB\n\n3. **Answer Generation**\n   - Construct prompt with retrieved context\n   - Format: \"Context: {chunks}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n   - Generate answer using LLM\n   - Return answer with sources and similarity scores\n\n### Phase 4: Evaluation & Metrics\n1. **Implement Metric Calculators**\n   - Context relevance scorer\n   - Answer relevance scorer\n   - Faithfulness checker\n   - Retrieval metrics (Hit Rate, MRR)\n\n2. **Logging & Output**\n   - Log queries, retrieved contexts, and answers\n   - Save evaluation metrics to file\n   - Create visualization of results (optional)\n\n3. **Test Cases**\n   - Create test questions for evaluation\n   - Compare results across different configurations\n\n---\n\n## Notes & Considerations\n\n### Performance Optimization:\n- Use batch processing for embeddings\n- Consider quantization (4-bit/8-bit) for LLM if memory constrained\n- Cache embeddings to avoid recomputation\n- Use GPU memory efficiently (offload when not in use)\n\n### Quality Improvements:\n- Experiment with different chunk sizes\n- Tune top-k retrieval parameter\n- Try different prompt templates\n- Consider re-ranking retrieved results\n\n### Future Enhancements:\n- Add query expansion/reformulation\n- Implement conversational memory for multi-turn QA\n- Add citation/source attribution in answers\n- Support multiple embedding models comparison\n- Web interface for easier interaction\n\n### Error Handling:\n- Handle missing PDFs gracefully\n- Validate embedding dimensions\n- Catch model loading errors\n- Log failures for debugging\n\n---\n\n## Dependencies\n```python\n# Core ML\ntorch\ntransformers\nsentence-transformers\n\n# Vector DB\nchromadb\n\n# Text Processing\npypdf or PyPDF2\nlangchain or langchain-text-splitters\nnltk\n\n# Evaluation\nscikit-learn (for metrics)\nnumpy\npandas\n\n# Optional\nragas (for advanced RAG metrics)\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ENVIRONMENT CONFIGURATION"
   ],
   "metadata": {
    "id": "IUnk5MPZJImN"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Install Required Packages\n# Run this cell first to install all dependencies",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!pip install -q transformers sentence-transformers torch accelerate\n!pip install -q chromadb\n!pip install -q pypdf langchain-text-splitters\n!pip install -q nltk scikit-learn pandas numpy\nprint(\"âœ“ All packages installed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Import Libraries",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Core ML libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nfrom sentence_transformers import SentenceTransformer\n\n# Vector database\nimport chromadb\nfrom chromadb.config import Settings\n\n# Text processing\nfrom pypdf import PdfReader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nimport nltk\n\n# Utilities\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\nimport json\nfrom datetime import datetime\nimport time\n\n# Download NLTK data for sentence tokenization\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt', quiet=True)\n\nprint(\"âœ“ All libraries imported successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Device Detection & Configuration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Detect device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Device: {device}\")\nif device.type == \"cuda\":\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\nelse:\n    print(\"Running on CPU - models will load slower\")\n\n# Configuration\nCONFIG = {\n    \"device\": device,\n    \"embedding_model_name\": \"BAAI/bge-small-en-v1.5\",\n    \"llm_model_name\": \"mistralai/Mistral-7B-Instruct-v0.2\",  # Can be changed based on resources\n    \"chunk_size\": 512,\n    \"chunk_overlap\": 100,\n    \"top_k\": 3,  # Number of chunks to retrieve\n    \"data_folder\": \"data\",\n    \"chroma_db_path\": \"./chroma_db\",\n    \"output_folder\": \"outputs\"\n}\n\nprint(\"\\nâœ“ Device detection complete!\")\nprint(f\"Configuration: {json.dumps({k: str(v) for k, v in CONFIG.items()}, indent=2)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create Folder Structure",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create necessary folders\nfolders = [CONFIG[\"data_folder\"], CONFIG[\"output_folder\"]]\n\nfor folder in folders:\n    Path(folder).mkdir(parents=True, exist_ok=True)\n    print(f\"âœ“ Created/verified folder: {folder}\")\n\n# Check if there are any PDFs in the data folder\npdf_files = list(Path(CONFIG[\"data_folder\"]).glob(\"*.pdf\"))\nprint(f\"\\nFound {len(pdf_files)} PDF file(s) in {CONFIG['data_folder']} folder\")\n\nif len(pdf_files) == 0:\n    print(f\"\\nâš  No PDF files found. Please add PDF files to the '{CONFIG['data_folder']}' folder before proceeding.\")\nelse:\n    print(\"PDF files:\")\n    for pdf in pdf_files:\n        print(f\"  - {pdf.name}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ingesting Data\n"
   ],
   "metadata": {
    "id": "8tMHfg4SJIwF"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Load PDFs and Extract Text",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def load_pdfs_from_folder(folder_path: str) -> List[Dict]:\n    \"\"\"\n    Load all PDF files from a folder and extract text with metadata\n    \n    Args:\n        folder_path: Path to folder containing PDF files\n        \n    Returns:\n        List of dictionaries containing document text and metadata\n    \"\"\"\n    documents = []\n    pdf_files = list(Path(folder_path).glob(\"*.pdf\"))\n    \n    if len(pdf_files) == 0:\n        print(f\"âš  No PDF files found in {folder_path}\")\n        return documents\n    \n    print(f\"Loading {len(pdf_files)} PDF file(s)...\")\n    \n    for pdf_path in pdf_files:\n        try:\n            reader = PdfReader(str(pdf_path))\n            num_pages = len(reader.pages)\n            \n            print(f\"\\n  Processing: {pdf_path.name} ({num_pages} pages)\")\n            \n            # Extract text from each page\n            for page_num, page in enumerate(reader.pages, start=1):\n                text = page.extract_text()\n                \n                if text.strip():  # Only add non-empty pages\n                    documents.append({\n                        \"text\": text,\n                        \"metadata\": {\n                            \"source\": pdf_path.name,\n                            \"page\": page_num,\n                            \"total_pages\": num_pages\n                        }\n                    })\n            \n            print(f\"    âœ“ Extracted {num_pages} pages\")\n                    \n        except Exception as e:\n            print(f\"    âœ— Error processing {pdf_path.name}: {str(e)}\")\n            continue\n    \n    print(f\"\\nâœ“ Successfully loaded {len(documents)} page(s) from {len(pdf_files)} PDF file(s)\")\n    return documents\n\n# Load all PDFs\ndocuments = load_pdfs_from_folder(CONFIG[\"data_folder\"])\n\n# Display summary\nif documents:\n    total_chars = sum(len(doc[\"text\"]) for doc in documents)\n    print(f\"\\nTotal characters extracted: {total_chars:,}\")\n    print(f\"Average characters per page: {total_chars // len(documents):,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process Data"
   ],
   "metadata": {
    "id": "FVN7SosbJJEn"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Chunk Documents with Semantic Splitting",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def chunk_documents(documents: List[Dict], chunk_size: int = 512, chunk_overlap: int = 100) -> List[Dict]:\n    \"\"\"\n    Chunk documents using semantic splitting with sentence boundaries\n    \n    Args:\n        documents: List of documents with text and metadata\n        chunk_size: Target size for each chunk\n        chunk_overlap: Number of characters to overlap between chunks\n        \n    Returns:\n        List of chunks with associated metadata\n    \"\"\"\n    # Initialize text splitter with sentence-aware splitting\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Prioritize semantic breaks\n        is_separator_regex=False\n    )\n    \n    chunks = []\n    \n    print(f\"Chunking {len(documents)} document(s)...\")\n    print(f\"  Chunk size: {chunk_size}, Overlap: {chunk_overlap}\\n\")\n    \n    for doc_idx, doc in enumerate(documents):\n        text = doc[\"text\"]\n        metadata = doc[\"metadata\"]\n        \n        # Split text into chunks\n        text_chunks = text_splitter.split_text(text)\n        \n        # Create chunk objects with metadata\n        for chunk_idx, chunk_text in enumerate(text_chunks):\n            chunks.append({\n                \"text\": chunk_text,\n                \"metadata\": {\n                    **metadata,  # Include original document metadata\n                    \"chunk_index\": chunk_idx,\n                    \"doc_index\": doc_idx,\n                    \"chunk_size\": len(chunk_text)\n                }\n            })\n    \n    print(f\"âœ“ Created {len(chunks)} chunks from {len(documents)} document(s)\")\n    print(f\"  Average chunk size: {sum(len(c['text']) for c in chunks) // len(chunks)} characters\")\n    \n    return chunks\n\n# Chunk the documents\nif documents:\n    chunks = chunk_documents(\n        documents, \n        chunk_size=CONFIG[\"chunk_size\"], \n        chunk_overlap=CONFIG[\"chunk_overlap\"]\n    )\n    \n    # Display some examples\n    print(f\"\\nðŸ“„ Example chunks:\")\n    for i, chunk in enumerate(chunks[:2]):  # Show first 2 chunks\n        print(f\"\\nChunk {i+1}:\")\n        print(f\"  Source: {chunk['metadata']['source']}, Page: {chunk['metadata']['page']}\")\n        print(f\"  Text preview: {chunk['text'][:150]}...\")\nelse:\n    print(\"âš  No documents to chunk. Please load PDFs first.\")\n    chunks = []",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save to vector database"
   ],
   "metadata": {
    "id": "QCi3ny5uLnNP"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Generate Embeddings and Store in ChromaDB",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def create_embeddings_and_store(chunks: List[Dict], model_name: str, device: torch.device, db_path: str) -> chromadb.Collection:\n    \"\"\"\n    Generate embeddings for chunks and store them in ChromaDB\n    \n    Args:\n        chunks: List of text chunks with metadata\n        model_name: Name of the embedding model\n        device: Device to use (CPU/GPU)\n        db_path: Path to ChromaDB storage\n        \n    Returns:\n        ChromaDB collection with stored embeddings\n    \"\"\"\n    if not chunks:\n        print(\"âš  No chunks to embed!\")\n        return None\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Loading embedding model: {model_name}\")\n    print(f\"{'='*60}\")\n    \n    # Load embedding model\n    embedding_model = SentenceTransformer(model_name, device=str(device))\n    print(f\"âœ“ Model loaded on {device}\")\n    \n    # Extract texts and prepare metadata\n    texts = [chunk[\"text\"] for chunk in chunks]\n    metadatas = [chunk[\"metadata\"] for chunk in chunks]\n    \n    # Convert metadata values to strings for ChromaDB compatibility\n    for metadata in metadatas:\n        for key, value in metadata.items():\n            metadata[key] = str(value)\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"Generating embeddings for {len(texts)} chunks...\")\n    print(f\"{'='*60}\")\n    \n    # Generate embeddings in batches for efficiency\n    batch_size = 32\n    embeddings = []\n    \n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        batch_embeddings = embedding_model.encode(\n            batch,\n            convert_to_numpy=True,\n            show_progress_bar=False\n        )\n        embeddings.extend(batch_embeddings.tolist())\n        \n        if (i // batch_size + 1) % 10 == 0:\n            print(f\"  Processed {i + len(batch)}/{len(texts)} chunks\")\n    \n    print(f\"âœ“ Generated {len(embeddings)} embeddings\")\n    print(f\"  Embedding dimension: {len(embeddings[0])}\")\n    \n    # Initialize ChromaDB client\n    print(f\"\\n{'='*60}\")\n    print(f\"Initializing ChromaDB...\")\n    print(f\"{'='*60}\")\n    \n    client = chromadb.PersistentClient(path=db_path)\n    \n    # Delete existing collection if it exists (for fresh start)\n    try:\n        client.delete_collection(\"rag_collection\")\n        print(\"  Deleted existing collection\")\n    except:\n        pass\n    \n    # Create new collection\n    collection = client.create_collection(\n        name=\"rag_collection\",\n        metadata={\"description\": \"RAG document chunks with embeddings\"}\n    )\n    \n    print(f\"âœ“ Created collection: rag_collection\")\n    \n    # Add documents to collection\n    print(f\"\\n{'='*60}\")\n    print(f\"Storing embeddings in ChromaDB...\")\n    print(f\"{'='*60}\")\n    \n    # ChromaDB requires unique IDs\n    ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n    \n    collection.add(\n        embeddings=embeddings,\n        documents=texts,\n        metadatas=metadatas,\n        ids=ids\n    )\n    \n    print(f\"âœ“ Successfully stored {collection.count()} chunks in vector database\")\n    print(f\"  Database path: {db_path}\")\n    \n    return collection, embedding_model\n\n# Generate embeddings and store in ChromaDB\nif chunks:\n    collection, embedding_model = create_embeddings_and_store(\n        chunks,\n        CONFIG[\"embedding_model_name\"],\n        CONFIG[\"device\"],\n        CONFIG[\"chroma_db_path\"]\n    )\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"âœ“ Phase 2 Complete: Data Ingestion & Processing\")\n    print(f\"{'='*60}\")\n    print(f\"  Total chunks embedded: {len(chunks)}\")\n    print(f\"  Vector database ready for querying\")\nelse:\n    print(\"âš  No chunks available. Please ensure PDFs are loaded and chunked first.\")\n    collection = None\n    embedding_model = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "M6nSuiYtL-VD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "tZHgW-JBL-XW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Models"
   ],
   "metadata": {
    "id": "C3avhryaL-l7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get User Question"
   ],
   "metadata": {
    "id": "-IcSSacRMGtJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Model"
   ],
   "metadata": {
    "id": "wVYR3nsQMHhO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Output"
   ],
   "metadata": {
    "id": "G5S08vRTMI3g"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Outputs"
   ],
   "metadata": {
    "id": "_sapiUdeMMm_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metrics"
   ],
   "metadata": {
    "id": "HcWpIrA9MM4c"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "P78NpzbYMFgh"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}